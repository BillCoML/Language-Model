{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import important libraries for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, losses, optimizers, layers, Model, mixed_precision\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings to train it with mixed float, the purpose is to reduce significantly the training time with a bit tradeoff for precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_logical_devices()\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "tokenizer = tiktoken.get_encoding(Config.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = layers.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position = self.Get_Position(config.context_length, config.d_model)\n",
    "\n",
    "    def Get_Position(self, context_length, d_model, n=10000):\n",
    "        P = np.zeros((context_length ,d_model))\n",
    "        for k in range(context_length):\n",
    "            for i in np.arange(int(d_model/2)):\n",
    "                denominator = np.power(n, 2*i/d_model)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i + 1] = np.cos(k/denominator)\n",
    "\n",
    "        return P\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.embed(inputs) + self.position\n",
    "\n",
    "################################################################################\n",
    "\n",
    "class Blocks(Layer):\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=config.num_heads,\n",
    "                                             key_dim = config.d_model)\n",
    "        self.ffn = Sequential([\n",
    "\n",
    "            layers.Dense(config.hidden_unit, activation = 'gelu'),\n",
    "\n",
    "            layers.Dropout(config.drop_out_rate),\n",
    "\n",
    "            layers.Dense(config.d_model),\n",
    "\n",
    "            layers.Dropout(config.drop_out_rate)\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "\n",
    "        self.add = layers.Add()\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        ##Multi-head Attention\n",
    "        attention_output = self.mha(\n",
    "            query = inputs,\n",
    "            key = inputs,\n",
    "            value = inputs,\n",
    "            use_causal_mask = True,\n",
    "        )\n",
    "\n",
    "        x = self.add([inputs, attention_output])\n",
    "\n",
    "        inputs2 = self.layernorm(x)\n",
    "\n",
    "        ##Feed Forward\n",
    "        x = self.ffn(inputs2)\n",
    "\n",
    "        x = self.add([x, inputs2])\n",
    "\n",
    "        return self.layernorm(x)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear = layers.Dense(config.vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT is the class to init a train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT():\n",
    "\n",
    "    def create_model(config = Config()):\n",
    "\n",
    "        return Sequential([\n",
    "\n",
    "            tf.keras.Input(shape=(config.context_length,)),\n",
    "\n",
    "            Embedding(config),\n",
    "\n",
    "            Sequential([\n",
    "                Blocks(config)\n",
    "                       for _ in range(config.block_count)\n",
    "                ]),\n",
    "\n",
    "            Linear(config),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model then show its architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GPT.create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train from pre-trained weights, we need to load those weights into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(Config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose optimizer, loss function and their hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate= 3e-4,  # Your initial learning rate\n",
    "    decay_steps = 10000,  # Number of steps to decay over\n",
    "    decay_rate = 0.9  # Decay rate per decay_steps\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "opt = optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "############ Compile #############################\n",
    "\n",
    "model.compile(\n",
    "    loss = loss_fn,\n",
    "    optimizer = opt,\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train data to x_train and y_train. \n",
    "\n",
    "Their dimension is (batch x context length)\n",
    "\n",
    "Since x_train and y_train differ in only 1 out of N columns, I compress them then extract them later to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train = np.load(\"/content/drive/MyDrive/data/xy_train.npy\")\n",
    "\n",
    "x_train = xy_train[:, :-1]\n",
    "y_train = xy_train[:,  1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up path so that model save the weights to that path when it is called\n",
    "\n",
    "We use .h5 file for weights saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Config.model_path\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "model.save_weights(model_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model.fit(x_train, y_train,\n",
    "        batch_size = batch_size,\n",
    "        epochs = 5,\n",
    "        callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
