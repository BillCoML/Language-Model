{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Latex, clear_output\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, losses, optimizers, layers, Model, mixed_precision\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "tokenizer = tiktoken.get_encoding(Config.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = layers.Embedding(config.vocab_size, config.d_model)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.embed(inputs)\n",
    "    \n",
    "def Get_Position(context_length, d_model, n=10000):\n",
    "    P = np.zeros((context_length ,d_model))\n",
    "    for k in range(context_length):\n",
    "        for i in np.arange(int(d_model/2)):\n",
    "            denominator = np.power(n, 2*i/d_model)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i + 1] = np.cos(k/denominator)\n",
    "    \n",
    "    return P\n",
    "\n",
    "class Blocks(Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=config.num_heads,\n",
    "                                             key_dim = config.d_model)\n",
    "        self.ffn = Sequential([\n",
    "            layers.Dense(config.hidden_unit, activation = 'gelu'),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.Dense(config.d_model),\n",
    "            layers.Dropout(0.1)\n",
    "        ])\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        ##Multi-head Attention\n",
    "        attention_output = self.mha(\n",
    "            query = inputs,\n",
    "            key = inputs,\n",
    "            value = inputs,\n",
    "            use_causal_mask = True,\n",
    "        )\n",
    "        x = self.add([inputs, attention_output])\n",
    "        inputs2 = self.layernorm(x)\n",
    "\n",
    "        ##Feed Forward\n",
    "        x = self.ffn(inputs2)\n",
    "        x = self.add([x, inputs2])\n",
    "        return self.layernorm(x)\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear = layers.Dense(config.vocab_size)\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs)\n",
    "    \n",
    "def create_model(config = Config()):   \n",
    "\n",
    "    return Sequential([\n",
    "\n",
    "        tf.keras.Input(shape=(None,)),\n",
    "\n",
    "        Embedding(config),\n",
    "\n",
    "        Sequential([\n",
    "            Blocks(config)\n",
    "                for _ in range(config.block_count)\n",
    "            ]),\n",
    "\n",
    "        Linear(config),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model_path = Config.model_path\n",
    "model.load_weights(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_prompt(prompt):\n",
    "    '''\n",
    "    A prompt must end with a dot.\n",
    "    A prompt should start with upper letter\n",
    "    '''\n",
    "    prompt = prompt.capitalize()\n",
    "\n",
    "    list = ['.', '?', '!']\n",
    "    if (prompt[-1] not in list):\n",
    "        prompt = prompt+'.'\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def init_inputs(human):\n",
    "\n",
    "    encoding = tokenizer.encode(human)\n",
    "    current_length = len(encoding)\n",
    "    try:\n",
    "        assert (current_length < Config.context_length)\n",
    "        gen_array = []\n",
    "        \n",
    "        for tokenIdx in range(current_length):\n",
    "            gen_array.append(encoding[tokenIdx])\n",
    "    \n",
    "        return gen_array, current_length\n",
    "\n",
    "    except AssertionError:\n",
    "        print(f\"({current_length})\")\n",
    "        print(\" >> Exceeds limit, chat ended.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def make_predict(x):\n",
    "    '''\n",
    "    Here we want to make different prediction every time\n",
    "    '''\n",
    "    _, cl = x.shape\n",
    "    \n",
    "    next = model.layers[0](x) + Get_Position(context_length=cl, \n",
    "                                             d_model=Config.d_model)\n",
    "\n",
    "    attention = model.layers[1](next)\n",
    "\n",
    "    logits = model.layers[-1](attention[:, -1, :])[0]\n",
    "\n",
    "    choice = np.argmax(logits)\n",
    "\n",
    "    return choice\n",
    "\n",
    "\n",
    "def check_prediction(gen_array, prediction):\n",
    "\n",
    "    legit = True\n",
    "    gen_array.append(prediction)\n",
    "    fed_in_array = np.array(gen_array).reshape(1, len(gen_array))\n",
    "    nxt_token = make_predict(fed_in_array)\n",
    "\n",
    "    if (nxt_token == 25):\n",
    "        legit = False\n",
    "        gen_array.pop()\n",
    "    \n",
    "    return legit\n",
    "\n",
    "\n",
    "def clear_and_reload(prompt, content):\n",
    "    clear_output()\n",
    "    display(Latex(f\"\\n*** Your Question:\\n\\n{prompt}\\n\"))\n",
    "    display(Latex(content))\n",
    "    \n",
    "\n",
    "########################################################################\n",
    "\n",
    "def generate(fix=False, reload_scaler=3):\n",
    "\n",
    "    prompt = str(input(\"\\n*** Your Question:\\n\\n\"))\n",
    "\n",
    "    if (fix):\n",
    "        prompt = fix_prompt(prompt)\n",
    "\n",
    "    #This is the actual part given to the model\n",
    "    generated = f\"\\n\\nHuman: {prompt}\\n\\nAssistant: \"\n",
    "\n",
    "    gen_array, current_length = init_inputs(generated)\n",
    "    \n",
    "    if gen_array == None:\n",
    "        return\n",
    "\n",
    "    ##Pre-process done, now generating ...\n",
    "    print('\\n ***Generated Answer:\\n')\n",
    "\n",
    "    #here I reuse prompt so it will be printed as LaTex in cell\n",
    "    reload = f\"*** Generated Answer:\\n\\n\"\n",
    "\n",
    "    while (current_length < Config.context_length):\n",
    "        \n",
    "        fed_in_array = np.array(gen_array).reshape(1, current_length)\n",
    "\n",
    "\n",
    "        prediction = make_predict(fed_in_array)\n",
    "\n",
    "        if (prediction == 50256):\n",
    "            #endoftext token\n",
    "            break\n",
    "\n",
    "        if (prediction in [20490, 48902]):\n",
    "            try:\n",
    "                legit = check_prediction(gen_array, prediction)\n",
    "                assert legit == True\n",
    "            except AssertionError:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            gen_array.append(prediction)\n",
    "        \n",
    "        nxt_token_ = tokenizer.decode([prediction])\n",
    "        '''\n",
    "        generated is for model to generate\n",
    "        reload is the contents we will see\n",
    "        '''\n",
    "        generated += nxt_token_\n",
    "        reload += nxt_token_\n",
    "        current_length += 1\n",
    "\n",
    "        if (current_length % reload_scaler == 0):\n",
    "            clear_and_reload(prompt, reload)\n",
    "\n",
    "    clear_and_reload(prompt, reload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(reload_scaler=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
